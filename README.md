# Cassandra: Real-time Sales Pipeline with Kafka, Spark & Cassandra
This project implements a real-time data pipeline that simulates sales events. The data is generated by a Python producer, sent to a Kafka topic, processed in real time using Apache Spark Structured Streaming, and stored in a Cassandra database.

## Objectives
1. Simulate a real-time sales event stream.

2. Process events in real time using Apache Spark.

3. Persist the results in a NoSQL database (Cassandra) for further analysis.

## Technologies Used:

- Apache Kafka: Message broker for data streaming.
- Docker: Containerization of services.
- Cassandra: Distributed NoSQL database.
- Apache Spark (PySpark): Distributed real-time data processing.
- Python: Data generation (producer) and pipeline management.

## **Setup and Execution**

### **Install and Run**
```bash
# Clone this repo
$ git clone https://github.com/marianelaruiz/cassandra-ecommerce-sales
# Enter the project folder
$ cd cassandra-ecommerce-sales
```

### 🔗 Kafka

### **1️⃣ Pull the Kafka Docker Image:**

Download the latest Kafka image from Docker Hub.

```bash
docker pull apache/kafka:4.0.0
```

### **2️⃣ Start the Kafka Docker Container:**

Run the Kafka container, exposing it on port `9092`.

```bash
docker run --name kafka-server -p 9092:9092 -d apache/kafka:4.0.0
```

### **3️⃣ Create a Kafka Topic:**

Create the topic `sales-events` inside the Kafka container.

```bash
docker exec -it kafka-server /opt/kafka/bin/kafka-topics.sh --create  --topic sales-events   --bootstrap-server localhost:9092
```

### **4️⃣ List Available Topics:**

Verify that the topic has been created successfully.

```bash
docker exec -it kafka-server /opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list
```

### 🗄️ Cassandra

### **1️⃣ Pull the Cassandra Docker image:**

```bash
docker pull cassandra:latest
```

### **2️⃣ Create a Docker network (if it does not exist):**

```bash
docker network create cassandra
```

### **3️⃣ Start the Cassandra container:**

```bash
docker run --rm -d \
--name cassandra \
--hostname cassandra \
--network cassandra \
-p 9042:9042 \
cassandra
```

### **4️⃣ Execute the CQL script to create keyspaces and tables:**

```bash
docker run --rm \
  --network cassandra \
  -v "$(pwd)/data.cql:/scripts/data.cql" \
  cassandra \
  cqlsh cassandra -f /scripts/data.cql
```

### 🐍 Python (Producer)

### **1️⃣ Create a virtual environment:**

***MacOS & Linux***

```bash
python3 -m venv venv
```

***Windows***
```bash
python -m venv venv 
```

### **2️⃣ Activate the environment:**

***MacOS & Linux***

```bash
source venv/bin/activate
```

***Windows***
```bash
venv\Scripts\activate
```


### **3️⃣ Install dependencies:**

```bash
pip install -r requirements.txt
```

### **4️⃣ Run the producer:**

```bash
python3 producer.py
```


### 🔥 PySpark (Consumer)

### **1️⃣ Run the consumer:**

```bash
spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.0 \
  consumer.py
```

### 📊 View data

```bash
# Enter the cassandra terminal
docker exec -it cassandra cqlsh
```

```bash
# Check Casandra Table
select * from ecommerce.sales;
```



##  Project Workflow

```bash
[ Producer (Python) ] ---> [ Kafka Topic: sales-events ] ---> [ Spark Streaming Consumer ] ---> [ Cassandra ]
```


## Conclusion:

This project demonstrates how to build a real-time data pipeline using modern distributed processing tools. It provides a practical example of how businesses can capture, process, and store sales events (or any other events) in real time, enabling data-driven decisions instantly.






---------------------------------------------------------
<!-- 
### **5️⃣ Set up the Environment:**

Clone the repository and create a virtual environment:

```bash
# Clone the repository
git clone https://github.com/marianelaruiz/kafka-ecommerce-sales.git
cd kafka-ecommerce-sales

# Create the virtual enviroment 
python3 -m venv kafka_venv

# Activate the virtual environment
source kafka_venv/bin/activate

# Install the dependencies
pip install -r requirements.txt

```

### **6️⃣ Run the Producer:**

Activate the virtual environment and start the producer to send data to the topic.

```bash
python3 producer.py
```

### **7️⃣ Run the Consumer:**

Execute the PySpark consumer to read and process the data from the Kafka topic.

```bash
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 consumer.py
```

### **8️⃣ Stop the Process:**
To stop any Kafka service, press Ctrl + C in the terminal.

---

 -->



